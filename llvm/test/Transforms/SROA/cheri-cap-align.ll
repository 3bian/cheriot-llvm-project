; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -mtriple=riscv64-unknown-freebsd -S -passes=sroa < %s | FileCheck %s

target datalayout = "E-m:e-pf200:128:128:128:64-i8:8:32-i16:16:32-i64:64-n32:64-S128"
%struct = type { i32, i32, i32, i32, ptr addrspace(200) }

; Reduced test case based on FreeBSD newsyslog's parseDWM.
; The i32 store means that the the first 16 bytes of the alloca are not tagged for the
; final memcpy, so we could replace everything with scalars.
define void @foo_struct(ptr %px) {
; CHECK-LABEL: @foo_struct(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[PY_SROA_0:%.*]] = alloca { i32, i32, i32, i32 }, align 16
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PY_SROA_0]], ptr align 16 [[PX:%.*]], i64 16, i1 false)
; CHECK-NEXT:    [[PY_SROA_6_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 16
; CHECK-NEXT:    [[PY_SROA_6_0_COPYLOAD:%.*]] = load ptr addrspace(200), ptr [[PY_SROA_6_0_PX_SROA_IDX]], align 16
; CHECK-NEXT:    [[PY_SROA_0_4_PY_1_SROA_IDX5:%.*]] = getelementptr inbounds i8, ptr [[PY_SROA_0]], i64 4
; CHECK-NEXT:    [[PY_SROA_0_4_PY_SROA_0_4_Y_1:%.*]] = load i32, ptr [[PY_SROA_0_4_PY_1_SROA_IDX5]], align 4
; CHECK-NEXT:    [[Y_1_NEW:%.*]] = call i32 @bar(i32 [[PY_SROA_0_4_PY_SROA_0_4_Y_1]])
; CHECK-NEXT:    [[PY_SROA_0_4_PY_1_SROA_IDX4:%.*]] = getelementptr inbounds i8, ptr [[PY_SROA_0]], i64 4
; CHECK-NEXT:    store i32 [[Y_1_NEW]], ptr [[PY_SROA_0_4_PY_1_SROA_IDX4]], align 4
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PX]], ptr align 16 [[PY_SROA_0]], i64 16, i1 false)
; CHECK-NEXT:    [[PY_SROA_6_0_PX_SROA_IDX2:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 16
; CHECK-NEXT:    store ptr addrspace(200) [[PY_SROA_6_0_COPYLOAD]], ptr [[PY_SROA_6_0_PX_SROA_IDX2]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %py = alloca %struct, align 16
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %py, ptr align 16 %px, i64 32, i1 false)
  %py.1 = getelementptr inbounds %struct, ptr %py, i64 0, i32 1
  %y.1 = load i32, ptr %py.1, align 4
  %y.1.new = call i32 @bar(i32 %y.1)
  store i32 %y.1.new, ptr %py.1, align 4
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %px, ptr align 16 %py, i64 32, i1 false)
  ret void
}

; This is the same as @foo_struct, but with an opaque aligned byte buffer
; instead of a struct. Without the type information (and without control flow
; analysis), we have to assume that the first half of the buffer could contain
; a capability too, and thus it cannot be further broken down by SROA alone.
define void @foo_buf(ptr %px) {
; CHECK-LABEL: @foo_buf(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[PY_SROA_0:%.*]] = alloca [16 x i8], align 16
; CHECK-NEXT:    [[PY_SROA_6:%.*]] = alloca [16 x i8], align 16
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PY_SROA_0]], ptr align 16 [[PX:%.*]], i64 16, i1 false)
; CHECK-NEXT:    [[PY_SROA_6_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 16
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PY_SROA_6]], ptr align 16 [[PY_SROA_6_0_PX_SROA_IDX]], i64 16, i1 false)
; CHECK-NEXT:    [[PY_SROA_0_4_PY_1_SROA_IDX4:%.*]] = getelementptr inbounds i8, ptr [[PY_SROA_0]], i64 4
; CHECK-NEXT:    [[PY_SROA_0_4_PY_SROA_0_4_Y_1:%.*]] = load i32, ptr [[PY_SROA_0_4_PY_1_SROA_IDX4]], align 4
; CHECK-NEXT:    [[Y_1_NEW:%.*]] = call i32 @bar(i32 [[PY_SROA_0_4_PY_SROA_0_4_Y_1]])
; CHECK-NEXT:    [[PY_SROA_0_4_PY_1_SROA_IDX3:%.*]] = getelementptr inbounds i8, ptr [[PY_SROA_0]], i64 4
; CHECK-NEXT:    store i32 [[Y_1_NEW]], ptr [[PY_SROA_0_4_PY_1_SROA_IDX3]], align 4
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PX]], ptr align 16 [[PY_SROA_0]], i64 16, i1 false)
; CHECK-NEXT:    [[PY_SROA_6_0_PX_SROA_IDX2:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 16
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PY_SROA_6_0_PX_SROA_IDX2]], ptr align 16 [[PY_SROA_6]], i64 16, i1 false)
; CHECK-NEXT:    ret void
;
entry:
  %py = alloca [32 x i8], align 16
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %py, ptr align 16 %px, i64 32, i1 false)
  %py.1 = getelementptr inbounds [32 x i8], ptr %py, i64 0, i64 4
  %y.1 = load i32, ptr %py.1, align 4
  %y.1.new = call i32 @bar(i32 %y.1)
  store i32 %y.1.new, ptr %py.1, align 4
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %px, ptr align 16 %py, i64 32, i1 false)
  ret void
}

; This is similar to @foo_struct, but with a memcpy at an unaligned offset that
; still covers an aligned region of the struct that can contain capabilities.
; Previously, SROA would allocate an unaligned buffer that could cause tags to
; be stripped.
define void @unaligned_copy_struct(ptr %px) {
; CHECK-LABEL: @unaligned_copy_struct(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[PY_SROA_2:%.*]] = alloca { i32, i32, i32 }, align 8
; CHECK-NEXT:    [[PY_SROA_0_0_COPYLOAD:%.*]] = load i32, ptr [[PX:%.*]], align 16
; CHECK-NEXT:    [[PY_SROA_2_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 4
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[PY_SROA_2]], ptr align 4 [[PY_SROA_2_0_PX_SROA_IDX]], i64 12, i1 false) #[[ATTR1:[0-9]+]]
; CHECK-NEXT:    [[PY_SROA_3_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 16
; CHECK-NEXT:    [[PY_SROA_3_0_COPYLOAD:%.*]] = load ptr addrspace(200), ptr [[PY_SROA_3_0_PX_SROA_IDX]], align 16
; CHECK-NEXT:    [[X_0:%.*]] = load i32, ptr [[PX]], align 4
; CHECK-NEXT:    [[Y_0:%.*]] = call i32 @bar(i32 [[X_0]])
; CHECK-NEXT:    [[PX_TAIL:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 4
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[PX_TAIL]], ptr align 8 [[PY_SROA_2]], i64 12, i1 false) #[[ATTR1]]
; CHECK-NEXT:    [[PY_SROA_3_4_PX_TAIL_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX_TAIL]], i64 12
; CHECK-NEXT:    store ptr addrspace(200) [[PY_SROA_3_0_COPYLOAD]], ptr [[PY_SROA_3_4_PX_TAIL_SROA_IDX]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %py = alloca %struct, align 16
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %py, ptr align 16 %px, i64 32, i1 false)
  %x.0 = load i32, ptr %px, align 4
  %y.0 = call i32 @bar(i32 %x.0)
  store i32 %y.0, ptr %py, align 4
  %px.tail = getelementptr inbounds i8, ptr %px, i64 4
  %py.tail = getelementptr inbounds i8, ptr %py, i64 4
  call void @llvm.memcpy.p0.p0.i64(ptr align 4 dereferenceable(28) %px.tail, ptr align 4 dereferenceable(28) %py.tail, i64 28, i1 false)
  ret void
}

; This is the same as @unaligned_copy_struct but with an opaque aligned byte
; buffer instead of a struct, like @foo_buf is to @foo_struct. We have to
; assume the aligned slices of the struct could contain a capability, but the
; first 16 bytes can be broken up still due to seeing the store of an i32.
; Previously, SROA would allocate an unaligned buffer that could cause tags to
; be stripped.
define void @unaligned_copy_buf(ptr %px) {
; CHECK-LABEL: @unaligned_copy_buf(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[PY_SROA_2:%.*]] = alloca [12 x i8], align 4
; CHECK-NEXT:    [[PY_SROA_3:%.*]] = alloca [16 x i8], align 16
; CHECK-NEXT:    [[PY_SROA_0_0_COPYLOAD:%.*]] = load i32, ptr [[PX:%.*]], align 16
; CHECK-NEXT:    [[PY_SROA_2_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 4
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[PY_SROA_2]], ptr align 4 [[PY_SROA_2_0_PX_SROA_IDX]], i64 12, i1 false) #[[ATTR1]]
; CHECK-NEXT:    [[PY_SROA_3_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 16
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PY_SROA_3]], ptr align 16 [[PY_SROA_3_0_PX_SROA_IDX]], i64 16, i1 false)
; CHECK-NEXT:    [[X_0:%.*]] = load i32, ptr [[PX]], align 4
; CHECK-NEXT:    [[Y_0:%.*]] = call i32 @bar(i32 [[X_0]])
; CHECK-NEXT:    [[PX_TAIL:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 4
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[PX_TAIL]], ptr align 4 [[PY_SROA_2]], i64 12, i1 false) #[[ATTR1]]
; CHECK-NEXT:    [[PY_SROA_3_4_PX_TAIL_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX_TAIL]], i64 12
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[PY_SROA_3_4_PX_TAIL_SROA_IDX]], ptr align 16 [[PY_SROA_3]], i64 16, i1 false)
; CHECK-NEXT:    ret void
;
entry:
  %py = alloca [32 x i8], align 16
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %py, ptr align 16 %px, i64 32, i1 false)
  %x.0 = load i32, ptr %px, align 4
  %y.0 = call i32 @bar(i32 %x.0)
  store i32 %y.0, ptr %py, align 4
  %px.tail = getelementptr inbounds i8, ptr %px, i64 4
  %py.tail = getelementptr inbounds i8, ptr %py, i64 4
  call void @llvm.memcpy.p0.p0.i64(ptr align 4 %px.tail, ptr align 4 %py.tail, i64 28, i1 false)
  ret void
}

define void @baz(ptr %px) {
; CHECK-LABEL: @baz(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[PY_SROA_31:%.*]] = alloca [16 x i8], align 16
; CHECK-NEXT:    [[PY_SROA_0_0_COPYLOAD:%.*]] = load i32, ptr [[PX:%.*]], align 16
; CHECK-NEXT:    [[PY_SROA_1_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 4
; CHECK-NEXT:    [[PY_SROA_1_0_COPYLOAD:%.*]] = load i32, ptr [[PY_SROA_1_0_PX_SROA_IDX]], align 4
; CHECK-NEXT:    [[PY_SROA_3_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 8
; CHECK-NEXT:    [[PY_SROA_3_0_COPYLOAD:%.*]] = load i64, ptr [[PY_SROA_3_0_PX_SROA_IDX]], align 8
; CHECK-NEXT:    [[PY_SROA_31_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 16
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PY_SROA_31]], ptr align 16 [[PY_SROA_31_0_PX_SROA_IDX]], i64 16, i1 false)
; CHECK-NEXT:    [[Y_1_NEW:%.*]] = call i32 @bar(i32 [[PY_SROA_1_0_COPYLOAD]])
; CHECK-NEXT:    [[PX_OUT_1:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 16
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PX_OUT_1]], ptr align 16 [[PY_SROA_31]], i64 16, i1 false)
; CHECK-NEXT:    ret void
;
entry:
  %py = alloca [32 x i8], align 16
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %py, ptr align 16 %px, i64 32, i1 false)
  %py.1 = getelementptr inbounds [32 x i8], ptr %py, i64 0, i64 4
  %y.1 = load i32, ptr %py.1, align 4
  %y.1.new = call i32 @bar(i32 %y.1)
  store i32 %y.1.new, ptr %py.1, align 4
  %py.out.1 = getelementptr inbounds i8, ptr %py, i64 16
  %px.out.1 = getelementptr inbounds i8, ptr %px, i64 16
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %px.out.1, ptr align 16 %py.out.1, i64 16, i1 false)
  ret void
}

define void @bat(ptr %px) {
; CHECK-LABEL: @bat(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[PY_SROA_4:%.*]] = alloca [24 x i8], align 8
; CHECK-NEXT:    [[PY_SROA_0_0_COPYLOAD1:%.*]] = load i32, ptr [[PX:%.*]], align 16
; CHECK-NEXT:    [[PY_SROA_2_0_PX_SROA_IDX2:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 4
; CHECK-NEXT:    [[PY_SROA_2_0_COPYLOAD3:%.*]] = load i32, ptr [[PY_SROA_2_0_PX_SROA_IDX2]], align 4
; CHECK-NEXT:    [[PY_SROA_4_0_PX_SROA_IDX4:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 8
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[PY_SROA_4]], ptr align 8 [[PY_SROA_4_0_PX_SROA_IDX4]], i64 24, i1 false)
; CHECK-NEXT:    [[Y_1_NEW:%.*]] = call i32 @bar(i32 [[PY_SROA_2_0_COPYLOAD3]])
; CHECK-NEXT:    [[PY_SROA_0_0_COPYLOAD:%.*]] = load i32, ptr [[PX]], align 16
; CHECK-NEXT:    [[PY_SROA_2_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 4
; CHECK-NEXT:    [[PY_SROA_2_0_COPYLOAD:%.*]] = load i32, ptr [[PY_SROA_2_0_PX_SROA_IDX]], align 4
; CHECK-NEXT:    [[PY_SROA_4_0_PX_SROA_IDX:%.*]] = getelementptr inbounds i8, ptr [[PX]], i64 8
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[PY_SROA_4]], ptr align 8 [[PY_SROA_4_0_PX_SROA_IDX]], i64 24, i1 false)
; CHECK-NEXT:    ret void
;
entry:
  %py = alloca [32 x i8], align 16
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %py, ptr align 16 %px, i64 32, i1 false)
  %py.1 = getelementptr inbounds [32 x i8], ptr %py, i64 0, i64 4
  %y.1 = load i32, ptr %py.1, align 4
  %y.1.new = call i32 @bar(i32 %y.1)
  store i32 %y.1.new, ptr %py.1, align 4
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %py, ptr align 16 %px, i64 32, i1 false)
  ret void
}

%quad = type { %pair, %pair }
%pair = type { ptr addrspace(200), ptr addrspace(200) }

; This was previously mis-optimised. SROA revisited the memcpy multiple times
; on one pass without updating its offset information, yielding GEPs with
; incorrect offsets that went out of bounds, causing the alloca to be regarded
; as unused and the entire function optimised away. Whilst that's a correct end
; result, the intermediate states were not, and it's not SROA's responsibility
; to realise that.
define void @offset_transfer() {
; CHECK-LABEL: @offset_transfer(
; CHECK-NEXT:    [[PQ_SROA_0:%.*]] = alloca [[PAIR:%.*]], align 16
; CHECK-NEXT:    [[PQ_SROA_1:%.*]] = alloca [[PAIR]], align 16
; CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[PQ_SROA_0]], ptr align 16 [[PQ_SROA_1]], i64 32, i1 false)
; CHECK-NEXT:    ret void
;
  %pq = alloca %quad, align 16
  %pq.1 = getelementptr inbounds %quad, ptr %pq, i64 0, i32 1
  call void @llvm.memcpy.p0.p0.i64(ptr align 16 %pq, ptr align 16 %pq.1, i64 32, i1 false)
  ret void
}

declare i32 @bar(i32)
declare void @llvm.memcpy.p0.p0.i64(ptr noalias nocapture writeonly, ptr noalias nocapture readonly, i64, i1 immarg)
